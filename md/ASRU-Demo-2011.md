# RNNLM - Recurrent Neural Network Language Modeling Toolkit
## Question
- perplexity
- BLEU score
## Abstract
- We present a freely available open-source toolkit for training recurrent neural network based language models. It can be easily used to improve existing **speech recognition** and **machine translation** systems.
## Introduction
- Many techniques were developed to beat n-grams, but the improvements came atthe cost of computational complexity. Moreover, the improvements were often reported for very basic systems, and after application to state-of-the-art setups with n-gram models trained on huge data sets, the improvements provided by many techniques vanished.
## Recurrent Neural Network
- TODO
## Basic Functionality
- The toolkit supports several functions, mostly for the basic language modeling operations: training RNN LM, training hash-based maximum entropy model(ME LM) and RNNME LM(jointly trained RNN and ME models).