# Sequence-discriminative training of deep neural networks
## Abstract
- Different sequence-discriminative criteria - maximum mutual information (MMI), minimum phone error (MPE), state-level minimum Bayes risk (sMBR), and boosted MMI - are compared. Starting from a competitive DNN baseline trained using cross-entropy, different sequence-discriminative criteria are shown to lower word error rates by 7-9% relative, on average.
## Introduction
- In this paper, we present a comparison of the different training criteria for DNNs on the standard 300-hour Switchboard conversational telephone speech task, which has also been used in.
## Acoustic modeling with DNNs
- In a DNN-HMM hybrid system, the DNN is trained to provide `posterior probability` estimates for the HMM states.