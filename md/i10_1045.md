# Recurrent neural network based language model
## Abstract
- A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. We provide ample empirical evidence to suggest that **connectionist language models are superior to standard n-gram techniques, except their high conputational complexity.**
## Introduction
- **cache models** : describe long context information
- **class-based models** : improve parameter estimation for short contexts by sharing parameters between similar words
- Language models for real-world speech recognition or machine translation systems are built on huge amounts of data, and popular belief says that more data is all we need.
## Model description
- In speech recognition experiments, history is represent by hypothesis given by recognizer, and conains recognition errors.